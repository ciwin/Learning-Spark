{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Optimizing and Tuning Spark Applications\n",
    "Christoph Windheuser    \n",
    "May, 2022   \n",
    "Python examples of chapter 7 (page 173 ff) in the book *Learning Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python spark libraries\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, expr, when, concat, lit, avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a SparkSession\n",
    "\n",
    "spark = (SparkSession \\\n",
    "         .builder \\\n",
    "         .enableHiveSupport() \\\n",
    "         .appName(\"Chapter_7\") \\\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark\r\n"
     ]
    }
   ],
   "source": [
    "# Show the content of the environment variable $SPARK_HOME:\n",
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all config files\n",
    "!ls -l $SPARK_HOME/conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get single Spark configuiration values:\n",
    "print(spark.conf.get(\"spark.sql.warehouse.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the whole confiuguration context of a Spark Context:\n",
    "scConf = sc.getConf().getAll()\n",
    "\n",
    "for l in scConf:\n",
    "    print (l[0] + \":\")\n",
    "    print (l[1])\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change single Spark config variables\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the Spark SQL-specifdic Spark configs:\n",
    "spark.sql(\"SET -v\").select(\"key\", \"value\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark's Web Interface\n",
    "To see Spark's Web Interface, go the web address: http://127.0.0.1:4040    \n",
    "The tab *Environment* shows all environment variables. In the web interface, the variables are *read-only*, they cannot be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set configuration variables in a Spark program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check, if a configuration variable is modifiable:\n",
    "\n",
    "# Example:\n",
    "spark.conf.isModifiable(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual value of the variable:\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variable to a new variable and check:\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it back to the old value:\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "Page 181 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a big DataFrame:\n",
    "numDF = spark.range(1000 * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default number of partitions of this DataFrame\n",
    "numDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now change the number of partitions to another value\n",
    "numDF = spark.range(1000 * 1000).repartition(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of partitions>:\n",
    "numDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching of Data\n",
    "Page 183 ff.\n",
    "\n",
    "Create a DataFrame with 10M records.  \n",
    "\n",
    "The time difference (approx. 10x faster) between *Count and load into cache*' and *Count in cache*\n",
    "can only be demonstrated when this code is run the first time in the notebook. In consecutive executions the DataFrame is already cached and there is basically no time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Create:                    1.442420 seconds\n",
      "Step 2 - Add Column:                0.086140 seconds\n",
      "Step 3 - Cache df:                  0.292966 seconds\n",
      "Step 4 - Count and load into cache: 3.301321 seconds\n",
      "Step 5 - Count in cache:            0.181523 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "df = spark.range(1 * 10000000).toDF(\"id\")\n",
    "end = time.time()\n",
    "print(\"Step 1 - Create:                    %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df = df.withColumn(\"square\", df.id * df.id)\n",
    "end = time.time()\n",
    "print(\"Step 2 - Add Column:                %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df.cache()\n",
    "end = time.time()\n",
    "print(\"Step 3 - Cache df:                  %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(\"Step 4 - Count and load into cache: %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(\"Step 5 - Count in cache:            %f seconds\" %(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Tables and Views in SQL\n",
    "It is also possible to cache tables of views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|10000000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"CACHE TABLE dfTable\")\n",
    "spark.sql(\"SELECT count(*) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistance of Data\n",
    "Page 184 ff\n",
    "\n",
    "Persistance of data is synonymous to caching data, but let you apecify how the data is persisted with the parameter `pyspark.StorageLevel.LEVEL`. \n",
    "\n",
    "As we have specified the persistance on disk only, the time difference is much lower compared to the example above (this time approx. 5x faster compared to 12x faster above). Under the link http://127.0.0.1:4040/ you can see that the data is persisted on disk and not on memory for all partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Create:                    0.019116 seconds\n",
      "Step 2 - Add Column:                0.006613 seconds\n",
      "Step 3 - Persist df DISK_ONLY:      0.031600 seconds\n",
      "Step 4 - Count and load into cache: 1.571908 seconds\n",
      "Step 5 - Count in cache:            0.259982 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df2 = spark.range(1 * 10000000).toDF(\"id\")\n",
    "end = time.time()\n",
    "print(\"Step 1 - Create:                    %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df2 = df.withColumn(\"square\", df2.id * df2.id)\n",
    "end = time.time()\n",
    "print(\"Step 2 - Add Column:                %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df2.persist(storageLevel=pyspark.StorageLevel.DISK_ONLY)\n",
    "end = time.time()\n",
    "print(\"Step 3 - Persist df DISK_ONLY:      %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df2.count()\n",
    "end = time.time()\n",
    "print(\"Step 4 - Count and load into cache: %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df2.count()\n",
    "end = time.time()\n",
    "print(\"Step 5 - Count in cache:            %f seconds\" %(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Sort Merge Join (SMJ)\n",
    "Page 189 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# Disable broadcast join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for two data frames\n",
    "states = ['AZ', 'CO', 'CA', 'TX', 'NY', 'MI']\n",
    "items  = ['SKU-0', 'SKU-1', 'SKU-2', 'SKU-3', 'SKU-4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "userDF = spark.range(1000000).toDF(\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|uid|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userDF.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "userDF = (userDF\n",
    "            .withColumn(\"login\", concat(lit(\"user_\"), expr(\"uid\")))\n",
    "            .withColumn(\"email\", concat(lit(\"user_\"), expr(\"uid\"), lit(\"@databricks.com\")))\n",
    "            .withColumn(\"user_state\", lit(states[randint(0, 5)]))          \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+----------+\n",
      "|uid| login|               email|user_state|\n",
      "+---+------+--------------------+----------+\n",
      "|  0|user_0|user_0@databricks...|        TX|\n",
      "|  1|user_1|user_1@databricks...|        TX|\n",
      "|  2|user_2|user_2@databricks...|        TX|\n",
      "|  3|user_3|user_3@databricks...|        TX|\n",
      "|  4|user_4|user_4@databricks...|        TX|\n",
      "+---+------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userDF.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF = spark.range(1000000).toDF(\"transaction_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|transaction_id|\n",
      "+--------------+\n",
      "|             0|\n",
      "|             1|\n",
      "|             2|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF = (orderDF\n",
    "            .withColumn(\"quantity\", orderDF.transaction_id)\n",
    "            .withColumn(\"users_id\", lit(randint(0, 10000)))\n",
    "            .withColumn(\"amount\",   orderDF.transaction_id * 2.0)\n",
    "            .withColumn(\"state\",    lit(states[randint(0,5)]))\n",
    "            .withColumn(\"items\",    lit(items[randint(0,4)]))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+------+-----+-----+\n",
      "|transaction_id|quantity|users_id|amount|state|items|\n",
      "+--------------+--------+--------+------+-----+-----+\n",
      "|             0|       0|     820|   0.0|   MI|SKU-4|\n",
      "|             1|       1|     820|   2.0|   MI|SKU-4|\n",
      "|             2|       2|     820|   4.0|   MI|SKU-4|\n",
      "|             3|       3|     820|   6.0|   MI|SKU-4|\n",
      "|             4|       4|     820|   8.0|   MI|SKU-4|\n",
      "+--------------+--------+--------+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "userOrderDF = orderDF.join(userDF, orderDF.users_id == userDF.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+------+-----+-----+---+--------+--------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount|state|items|uid|   login|               email|user_state|\n",
      "+--------------+--------+--------+------+-----+-----+---+--------+--------------------+----------+\n",
      "|             0|       0|     820|   0.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             1|       1|     820|   2.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             2|       2|     820|   4.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             3|       3|     820|   6.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             4|       4|     820|   8.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             5|       5|     820|  10.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             6|       6|     820|  12.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             7|       7|     820|  14.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             8|       8|     820|  16.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|             9|       9|     820|  18.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            10|      10|     820|  20.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            11|      11|     820|  22.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            12|      12|     820|  24.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            13|      13|     820|  26.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            14|      14|     820|  28.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            15|      15|     820|  30.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            16|      16|     820|  32.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            17|      17|     820|  34.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            18|      18|     820|  36.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|            19|      19|     820|  38.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "+--------------+--------+--------+------+-----+-----+---+--------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userOrderDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CartesianProduct\n",
      ":- *(1) Project [id#38L AS transaction_id#40L, id#38L AS quantity#47L, 820 AS users_id#50, (cast(id#38L as double) * 2.0) AS amount#54, MI AS state#59, SKU-4 AS items#65]\n",
      ":  +- *(1) Range (0, 1000000, step=1, splits=16)\n",
      "+- *(2) Project [id#0L AS uid#2L, concat(user_, cast(id#0L as string)) AS login#9, concat(user_, cast(id#0L as string), @databricks.com) AS email#12, TX AS user_state#16]\n",
      "   +- *(2) Filter (820 = id#0L)\n",
      "      +- *(2) Range (0, 1000000, step=1, splits=16)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userOrderDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(userDF\n",
    "     .orderBy(col(\"uid\").asc())\n",
    "     .write.format(\"parquet\")\n",
    "     .bucketBy(8, \"uid\")\n",
    "     .mode(\"overWrite\")\n",
    "     .saveAsTable(\"UserTbl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orderDF\n",
    "     .orderBy(col(\"users_id\").asc())\n",
    "     .write.format(\"parquet\")\n",
    "     .bucketBy(8, \"users_id\")\n",
    "     .mode(\"overWrite\")\n",
    "     .saveAsTable(\"OrderTbl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CACHE TABLE UserTbl\")\n",
    "spark.sql(\"CACHE TABLE OrderTbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "userBucketDF  = spark.table(\"UserTbl\")\n",
    "orderBucketDF = spark.table(\"OrderTbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinUserOrderBucketDF = orderBucketDF.join(userBucketDF, orderBucketDF.users_id == userBucketDF.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+---------+-----+-----+---+--------+--------------------+----------+\n",
      "|transaction_id|quantity|users_id|   amount|state|items|uid|   login|               email|user_state|\n",
      "+--------------+--------+--------+---------+-----+-----+---+--------+--------------------+----------+\n",
      "|        750000|  750000|     820|1500000.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750001|  750001|     820|1500002.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750002|  750002|     820|1500004.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750003|  750003|     820|1500006.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750004|  750004|     820|1500008.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750005|  750005|     820|1500010.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750006|  750006|     820|1500012.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750007|  750007|     820|1500014.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750008|  750008|     820|1500016.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750009|  750009|     820|1500018.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750010|  750010|     820|1500020.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750011|  750011|     820|1500022.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750012|  750012|     820|1500024.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750013|  750013|     820|1500026.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750014|  750014|     820|1500028.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750015|  750015|     820|1500030.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750016|  750016|     820|1500032.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750017|  750017|     820|1500034.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750018|  750018|     820|1500036.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "|        750019|  750019|     820|1500038.0|   MI|SKU-4|820|user_820|user_820@databric...|        TX|\n",
      "+--------------+--------+--------+---------+-----+-----+---+--------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUserOrderBucketDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [cast(users_id#331 as bigint)], [uid#192L], Inner\n",
      "   :- Sort [cast(users_id#331 as bigint) ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(cast(users_id#331 as bigint), 200), ENSURE_REQUIREMENTS, [id=#394]\n",
      "   :     +- Filter isnotnull(users_id#331)\n",
      "   :        +- Scan In-memory table OrderTbl [transaction_id#329L, quantity#330L, users_id#331, amount#332, state#333, items#334], [isnotnull(users_id#331)]\n",
      "   :              +- InMemoryRelation [transaction_id#329L, quantity#330L, users_id#331, amount#332, state#333, items#334], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                    +- *(1) ColumnarToRow\n",
      "   :                       +- FileScan parquet default.ordertbl[transaction_id#329L,quantity#330L,users_id#331,amount#332,state#333,items#334] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/christoph/Dev/LearningSpark/spark-warehouse/ordertbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:bigint,quantity:bigint,users_id:int,amount:double,state:string,items:string>, SelectedBucketsCount: 8 out of 8\n",
      "   +- Sort [uid#192L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(uid#192L, 200), ENSURE_REQUIREMENTS, [id=#395]\n",
      "         +- Filter isnotnull(uid#192L)\n",
      "            +- Scan In-memory table UserTbl [uid#192L, login#193, email#194, user_state#195], [isnotnull(uid#192L)]\n",
      "                  +- InMemoryRelation [uid#192L, login#193, email#194, user_state#195], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- *(1) ColumnarToRow\n",
      "                           +- FileScan parquet default.usertbl[uid#192L,login#193,email#194,user_state#195] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/christoph/Dev/LearningSpark/spark-warehouse/usertbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<uid:bigint,login:string,email:string,user_state:string>, SelectedBucketsCount: 8 out of 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUserOrderBucketDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
