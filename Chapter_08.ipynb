{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d82f8f",
   "metadata": {},
   "source": [
    "# Chapter 8: Structured Streaming\n",
    "Christoph Windheuser    \n",
    "May, 2022   \n",
    "Python examples of chapter 8 (page 207 ff) in the book *Learning Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dbb8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python spark libraries\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.streaming import StreamingContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984cbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "# This requires access to the internet. If executed offline, an error is thrown\n",
    "\n",
    "spark = (SparkSession \\\n",
    "         .builder \\\n",
    "         .appName(\"Chapter_8\") \\\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2c9c4",
   "metadata": {},
   "source": [
    "# Create a stream\n",
    "Run the command `nc -lk 9999`in a terminal window.    \n",
    "All text you type into the terminal will be send as a data stream to port 9999 whenever you hit `Return`.    \n",
    "`nc` stands for *Netcat* and is a simple computer network utility available under Linux, macOS and Windows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6374c",
   "metadata": {},
   "source": [
    "# Example of Reading a stream of data\n",
    "Creating a DataFrame from a text data stream to be received over a socket connection on localhost. Doing a continuous word count on the streaming data and print the results to the console.\n",
    "\n",
    "Whenever text is typed into the `nc` command in the terminal, the text is processed and the word count is printed out in the console of the Jupyter notebook until the spark command `streamingQuery.stop()` is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37710fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = (spark\n",
    "         .readStream.format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", \"9999\")\n",
    "         .load()\n",
    ")\n",
    "\n",
    "words  = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "\n",
    "# The directory is created if it does not exist\n",
    "checkpointDir = \"/tmp/checkpoints\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19b38b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery = (counts\n",
    "                 .writeStream\n",
    "                 .format(\"console\")\n",
    "                 .outputMode(\"complete\")  \n",
    "                 .trigger(processingTime=\"2 second\")\n",
    "                 .option(\"checkpointLocation\", checkpointDir)\n",
    "                 .start()\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d4d5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Streaming Query:\n",
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8b29d",
   "metadata": {},
   "source": [
    "## Monitoring an Active Stream\n",
    "Page 223 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5153c8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '89ca0050-b240-4dd2-879d-94c053fb5212',\n",
       " 'runId': '59c7b124-9d56-4086-9302-941a05a37229',\n",
       " 'name': None,\n",
       " 'timestamp': '2022-05-21T20:06:04.004Z',\n",
       " 'batchId': 55,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 0, 'triggerExecution': 0},\n",
       " 'stateOperators': [{'operatorName': 'stateStoreSave',\n",
       "   'numRowsTotal': 39,\n",
       "   'numRowsUpdated': 0,\n",
       "   'allUpdatesTimeMs': 59,\n",
       "   'numRowsRemoved': 0,\n",
       "   'allRemovalsTimeMs': 0,\n",
       "   'commitTimeMs': 15712,\n",
       "   'memoryUsedBytes': 97864,\n",
       "   'numRowsDroppedByWatermark': 0,\n",
       "   'numShufflePartitions': 200,\n",
       "   'numStateStoreInstances': 200,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 600,\n",
       "    'loadedMapCacheMissCount': 200,\n",
       "    'stateOnCurrentVersionSizeBytes': 30952}}],\n",
       " 'sources': [{'description': 'TextSocketV2[host: localhost, port: 9999]',\n",
       "   'startOffset': 0,\n",
       "   'endOffset': 0,\n",
       "   'latestOffset': 0,\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@2db30221',\n",
       "  'numOutputRows': 0}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show last progress of the stream. Only shows results when the stream is active\n",
    "streamingQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ab88f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the actual status of the stream:\n",
    "streamingQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272d8ec",
   "metadata": {},
   "source": [
    "## Another Example\n",
    "See https://spark.apache.org/docs/latest/streaming-programming-guide.html\n",
    "and:\n",
    "https://github.com/apache/spark/blob/v3.2.1/examples/src/main/python/streaming/network_wordcount.py\n",
    "\n",
    "The Python code is in the file `network_wordcount.py`.\n",
    "\n",
    "1. Run the program `nc -lk 9999` in a terminal.    \n",
    "   This program sends all text entered in the terminal out via port 9999\n",
    "2. Run the command `spark-submit network_wordcount.py localhost 9999` in another terminal.\n",
    "3. Each time words are typed in the first terminal, the words are counted in the second terminal\n",
    "4. Terminate both commands with `Cntr-C`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ecf95",
   "metadata": {},
   "source": [
    "# Streaming Data Sources and Sinks\n",
    "Page 226 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ae2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dir /tmp/sparkInputDir\n",
    "\n",
    "inputDirectoryOfFiles = \"/tmp/sparkInputDir\"\n",
    "\n",
    "fileSchema = (StructType()\n",
    "             .add(StructField(\"key\", IntegerType()))\n",
    "             .add(StructField(\"value\", StringType()))\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "533cd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF =(spark\n",
    "          .readStream\n",
    "          .format(\"csv\")\n",
    "          .schema(fileSchema)\n",
    "          .option(\"maxFilesPerTrigger\", 1)\n",
    "          .load(inputDirectoryOfFiles)\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a379773e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "087c0c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b26e04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery = (inputDF\n",
    "                 .writeStream\n",
    "                 .format(\"console\")\n",
    "                 .outputMode(\"append\")\n",
    "                 .start()\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a10f2",
   "metadata": {},
   "source": [
    "Now copy the files `streamingData_xx.csv` one by one from the directory `data/streamingData` to the direcory `/tmp/sparkInputDir`. Everytime a file is copied, the content is read by the `readStream` and written to the console by the `writeStream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c6eddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop streaming\n",
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9db11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
