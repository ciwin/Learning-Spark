{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Optimizing and Tuning Spark Applications\n",
    "Christoph Windheuser    \n",
    "May, 2022   \n",
    "Python examples of chapter 7 (page 173 ff) in the book *Learning Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python spark libraries\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, expr, when, concat, lit, avg\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a SparkSession\n",
    "\n",
    "spark = (SparkSession \\\n",
    "         .builder \\\n",
    "         .enableHiveSupport() \\\n",
    "         .appName(\"Chapter_7\") \\\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the content of the environment variable $SPARK_HOME:\n",
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all config files\n",
    "!ls -l $SPARK_HOME/conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get single Spark configuiration values:\n",
    "print(spark.conf.get(\"spark.sql.warehouse.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the whole confiuguration context of a Spark Context:\n",
    "scConf = sc.getConf().getAll()\n",
    "\n",
    "for l in scConf:\n",
    "    print (l[0] + \":\")\n",
    "    print (l[1])\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change single Spark config variables\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the Spark SQL-specifdic Spark configs:\n",
    "spark.sql(\"SET -v\").select(\"key\", \"value\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark's Web Interface\n",
    "To see Spark's Web Interface, go the web address: http://127.0.0.1:4040    \n",
    "The tab *Environment* shows all environment variables. In the web interface, the variables are *read-only*, they cannot be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set configuration variables in a Spark program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check, if a configuration variable is modifiable:\n",
    "\n",
    "# Example:\n",
    "spark.conf.isModifiable(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual value of the variable:\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variable to a new variable and check:\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it back to the old value:\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "Page 181 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a big DataFrame:\n",
    "numDF = spark.range(1000 * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default number of partitions of this DataFrame\n",
    "numDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now change the number of partitions to another value\n",
    "numDF = spark.range(1000 * 1000).repartition(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of partitions>:\n",
    "numDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching of Data\n",
    "Page 183 ff.\n",
    "\n",
    "Create a DataFrame with 10M records.  \n",
    "\n",
    "The time difference (approx. 10x faster) between *Count and load into cache*' and *Count in cache*\n",
    "can only be demonstrated when this code is run the first time in the notebook. In consecutive executions the DataFrame is already cached and there is basically no time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "df = spark.range(1 * 10000000).toDF(\"id\")\n",
    "end = time.time()\n",
    "print(\"Step 1 - Create:                    %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df = df.withColumn(\"square\", df.id * df.id)\n",
    "end = time.time()\n",
    "print(\"Step 2 - Add Column:                %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df.cache()\n",
    "end = time.time()\n",
    "print(\"Step 3 - Cache df:                  %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(\"Step 4 - Count and load into cache: %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(\"Step 5 - Count in cache:            %f seconds\" %(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Tables and Views in SQL\n",
    "It is also possible to cache tables of views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"CACHE TABLE dfTable\")\n",
    "spark.sql(\"SELECT count(*) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistance of Data\n",
    "Page 184 ff\n",
    "\n",
    "Persistance of data is synonymous to caching data, but let you apecify how the data is persisted with the parameter `pyspark.StorageLevel.LEVEL`. \n",
    "\n",
    "As we have specified the persistance on disk only, the time difference is much lower compared to the example above (this time approx. 5x faster compared to 12x faster above). Under the link http://127.0.0.1:4040/ you can see that the data is persisted on disk and not on memory for all partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df2 = spark.range(1 * 10000000).toDF(\"id\")\n",
    "end = time.time()\n",
    "print(\"Step 1 - Create:                    %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df2 = df2.withColumn(\"square\", df2.id * df2.id)\n",
    "end = time.time()\n",
    "print(\"Step 2 - Add Column:                %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df2.persist(storageLevel=pyspark.StorageLevel.DISK_ONLY)\n",
    "end = time.time()\n",
    "print(\"Step 3 - Persist df DISK_ONLY:      %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df2.count()\n",
    "end = time.time()\n",
    "print(\"Step 4 - Count and load into cache: %f seconds\" %(end - start))\n",
    "\n",
    "start = time.time()\n",
    "df2.count()\n",
    "end = time.time()\n",
    "print(\"Step 5 - Count in cache:            %f seconds\" %(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Sort Merge Join (SMJ)\n",
    "Page 189 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# Disable broadcast join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for two data frames\n",
    "states = ['AZ', 'CO', 'CA', 'TX', 'NY', 'MI']\n",
    "items  = ['SKU-0', 'SKU-1', 'SKU-2', 'SKU-3', 'SKU-4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF_schema = StructType([ \\\n",
    "    StructField(\"uid\",StringType(),True), \\\n",
    "    StructField(\"login\",StringType(),True), \\\n",
    "    StructField(\"email\",StringType(),True), \\\n",
    "    StructField(\"user_state\", StringType(), True) \\\n",
    "  ])\n",
    "\n",
    "ordersDF_schema = StructType([ \\\n",
    "    StructField(\"transaction_id\",StringType(),True), \\\n",
    "    StructField(\"quantity\",StringType(),True), \\\n",
    "    StructField(\"users_id\",StringType(),True), \\\n",
    "    StructField(\"amount\", StringType(), True), \\\n",
    "    StructField(\"state\", StringType(), True), \\\n",
    "    StructField(\"items\", StringType(), True) \\\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF_data = []\n",
    "for i in range (100000):\n",
    "    login = \"user_{}\".format(i)\n",
    "    usersDF_row = (str(i), login, login + \"@databricks.com\",\n",
    "                    states[randint(0, 5)])\n",
    "    usersDF_data.append(usersDF_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.createDataFrame(data=usersDF_data, schema=usersDF_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+----------+\n",
      "|uid| login|               email|user_state|\n",
      "+---+------+--------------------+----------+\n",
      "|  0|user_0|user_0@databricks...|        MI|\n",
      "|  1|user_1|user_1@databricks...|        CA|\n",
      "|  2|user_2|user_2@databricks...|        CO|\n",
      "|  3|user_3|user_3@databricks...|        NY|\n",
      "|  4|user_4|user_4@databricks...|        NY|\n",
      "|  5|user_5|user_5@databricks...|        CO|\n",
      "|  6|user_6|user_6@databricks...|        CO|\n",
      "|  7|user_7|user_7@databricks...|        MI|\n",
      "|  8|user_8|user_8@databricks...|        AZ|\n",
      "|  9|user_9|user_9@databricks...|        CA|\n",
      "+---+------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF_data = []\n",
    "for i in range (100000):\n",
    "    login = \"user_{}\".format(i)\n",
    "    ordersDF_row = (str(randint(100000, 999999)), \n",
    "                   str(randint(1, 100)),\n",
    "                   str(randint(0, 9999)),\n",
    "                   str((randint(10, 9999)/3.14)),\n",
    "                   states[randint(0, 5)],\n",
    "                   items[randint(0, 4)])\n",
    "    ordersDF_data.append(ordersDF_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF = spark.createDataFrame(data=ordersDF_data,\n",
    "                                 schema=ordersDF_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+------------------+-----+-----+\n",
      "|transaction_id|quantity|users_id|            amount|state|items|\n",
      "+--------------+--------+--------+------------------+-----+-----+\n",
      "|        316601|       6|    5767|3065.9235668789806|   MI|SKU-1|\n",
      "|        188678|      81|    3768|2654.7770700636943|   TX|SKU-1|\n",
      "|        232450|      65|    7670|2457.9617834394903|   CO|SKU-3|\n",
      "|        104382|      11|    9711| 2914.012738853503|   TX|SKU-1|\n",
      "|        183808|      26|    7283|1225.4777070063694|   TX|SKU-4|\n",
      "|        495994|      56|     808|1615.2866242038217|   MI|SKU-1|\n",
      "|        455218|      93|    2538| 59.23566878980891|   CO|SKU-2|\n",
      "|        129649|      67|     900| 2325.796178343949|   TX|SKU-3|\n",
      "|        331143|      64|    5184| 605.0955414012739|   TX|SKU-1|\n",
      "|        653319|      18|    7037|414.01273885350315|   CO|SKU-0|\n",
      "+--------------+--------+--------+------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersOrdersDF = ordersDF.join(usersDF, ordersDF.users_id == usersDF.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+------------------+-----+-----+----+---------+--------------------+----------+\n",
      "|transaction_id|quantity|users_id|            amount|state|items| uid|    login|               email|user_state|\n",
      "+--------------+--------+--------+------------------+-----+-----+----+---------+--------------------+----------+\n",
      "|        787788|      60|    1008|1288.8535031847134|   TX|SKU-4|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        947098|      27|    1008|3172.9299363057326|   TX|SKU-2|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        802590|       5|    1008| 3167.515923566879|   NY|SKU-0|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        327971|       3|    1008| 1220.063694267516|   CA|SKU-1|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        284095|      16|    1008| 618.7898089171974|   CO|SKU-3|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        783847|      39|    1008|2329.2993630573246|   MI|SKU-3|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        712740|      20|    1008|1309.2356687898089|   TX|SKU-1|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        851119|      11|    1008|  85.9872611464968|   MI|SKU-2|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        687369|      48|    1008|1000.3184713375796|   AZ|SKU-3|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        465402|      11|    1008| 3163.057324840764|   TX|SKU-3|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        148737|      36|    1008| 957.0063694267516|   CO|SKU-1|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        411220|      63|    1008|1042.3566878980891|   NY|SKU-0|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        991000|      87|    1008| 461.4649681528662|   MI|SKU-1|1008|user_1008|user_1008@databri...|        MI|\n",
      "|        219149|      45|     101|1036.3057324840763|   AZ|SKU-0| 101| user_101|user_101@databric...|        AZ|\n",
      "|        660877|      49|     101|2447.1337579617834|   AZ|SKU-4| 101| user_101|user_101@databric...|        AZ|\n",
      "|        862661|      21|     101| 429.9363057324841|   CO|SKU-0| 101| user_101|user_101@databric...|        AZ|\n",
      "|        366689|      13|     101| 2947.452229299363|   MI|SKU-1| 101| user_101|user_101@databric...|        AZ|\n",
      "|        766969|      56|     101| 3114.012738853503|   NY|SKU-2| 101| user_101|user_101@databric...|        AZ|\n",
      "|        783294|      72|     101| 311.4649681528662|   CA|SKU-2| 101| user_101|user_101@databric...|        AZ|\n",
      "|        817626|      26|     101|254.45859872611464|   NY|SKU-0| 101| user_101|user_101@databric...|        AZ|\n",
      "+--------------+--------+--------+------------------+-----+-----+----+---------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [users_id#27], [uid#0], Inner\n",
      "   :- Sort [users_id#27 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(users_id#27, 200), ENSURE_REQUIREMENTS, [id=#136]\n",
      "   :     +- Filter isnotnull(users_id#27)\n",
      "   :        +- Scan ExistingRDD[transaction_id#25,quantity#26,users_id#27,amount#28,state#29,items#30]\n",
      "   +- Sort [uid#0 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(uid#0, 200), ENSURE_REQUIREMENTS, [id=#137]\n",
      "         +- Filter isnotnull(uid#0)\n",
      "            +- Scan ExistingRDD[uid#0,login#1,email#2,user_state#3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Shuffle Sort Merge Join\n",
    "Page 193 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(usersDF\n",
    "     .orderBy(col(\"uid\").asc())\n",
    "     .write.format(\"parquet\")\n",
    "     .bucketBy(8, \"uid\")\n",
    "     .mode(\"overWrite\")\n",
    "     .saveAsTable(\"UserTbl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ordersDF\n",
    "     .orderBy(col(\"users_id\").asc())\n",
    "     .write.format(\"parquet\")\n",
    "     .bucketBy(8, \"users_id\")\n",
    "     .mode(\"overWrite\")\n",
    "     .saveAsTable(\"OrderTbl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CACHE TABLE UserTbl\")\n",
    "spark.sql(\"CACHE TABLE OrderTbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "userBucketDF  = spark.table(\"UserTbl\")\n",
    "orderBucketDF = spark.table(\"OrderTbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinUserOrderBucketDF = orderBucketDF.join(userBucketDF, orderBucketDF.users_id == userBucketDF.uid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+------------------+-----+-----+----+---------+--------------------+----------+\n",
      "|transaction_id|quantity|users_id|            amount|state|items| uid|    login|               email|user_state|\n",
      "+--------------+--------+--------+------------------+-----+-----+----+---------+--------------------+----------+\n",
      "|        555381|      49|       1| 678.9808917197452|   CA|SKU-2|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        729101|      17|       1| 721.3375796178344|   CO|SKU-3|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        213429|      76|       1| 1529.936305732484|   CA|SKU-3|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        809826|      24|       1|2372.6114649681526|   CA|SKU-4|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        504720|      30|       1|447.77070063694265|   NY|SKU-1|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        336000|      66|       1| 575.4777070063694|   AZ|SKU-1|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        965708|     100|       1|2713.6942675159235|   AZ|SKU-3|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        376891|       7|       1|  64.3312101910828|   CA|SKU-3|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        108082|      32|       1|2151.2738853503183|   TX|SKU-2|   1|   user_1|user_1@databricks...|        CA|\n",
      "|        559094|      24|    1003|2949.3630573248406|   CO|SKU-0|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        692241|      90|    1003|1551.2738853503183|   NY|SKU-2|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        652785|       8|    1003|1878.9808917197452|   NY|SKU-4|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        131933|      55|    1003|2523.5668789808915|   TX|SKU-1|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        928738|      19|    1003| 544.2675159235669|   AZ|SKU-2|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        632143|      62|    1003| 2595.541401273885|   CO|SKU-4|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        423175|      72|    1003|1570.7006369426751|   CO|SKU-0|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        292067|      38|    1003| 2438.853503184713|   CA|SKU-4|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        679984|      39|    1003| 2004.140127388535|   TX|SKU-1|1003|user_1003|user_1003@databri...|        CA|\n",
      "|        531409|      92|    1013|1687.8980891719746|   CO|SKU-4|1013|user_1013|user_1013@databri...|        NY|\n",
      "|        902704|      63|    1013| 1381.528662420382|   TX|SKU-1|1013|user_1013|user_1013@databri...|        NY|\n",
      "+--------------+--------+--------+------------------+-----+-----+----+---------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUserOrderBucketDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [users_id#292], [uid#153], Inner\n",
      "   :- Sort [users_id#292 ASC NULLS FIRST], false, 0\n",
      "   :  +- Filter isnotnull(users_id#292)\n",
      "   :     +- Scan In-memory table OrderTbl [transaction_id#290, quantity#291, users_id#292, amount#293, state#294, items#295], [isnotnull(users_id#292)]\n",
      "   :           +- InMemoryRelation [transaction_id#290, quantity#291, users_id#292, amount#293, state#294, items#295], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- *(1) ColumnarToRow\n",
      "   :                    +- FileScan parquet default.ordertbl[transaction_id#290,quantity#291,users_id#292,amount#293,state#294,items#295] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/cwi/Dev/LearningSpark/spark-warehouse/ordertbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:string,quantity:string,users_id:string,amount:string,state:string,items:str..., SelectedBucketsCount: 8 out of 8\n",
      "   +- Sort [uid#153 ASC NULLS FIRST], false, 0\n",
      "      +- Filter isnotnull(uid#153)\n",
      "         +- Scan In-memory table UserTbl [uid#153, login#154, email#155, user_state#156], [isnotnull(uid#153)]\n",
      "               +- InMemoryRelation [uid#153, login#154, email#155, user_state#156], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet default.usertbl[uid#153,login#154,email#155,user_state#156] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/cwi/Dev/LearningSpark/spark-warehouse/usertbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<uid:string,login:string,email:string,user_state:string>, SelectedBucketsCount: 8 out of 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUserOrderBucketDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization in Spark UI\n",
    "1. Go to http://localhost:4040\n",
    "2. Under the *Jobs* tab, click on the latest job\n",
    "3. Open *DAG Visualization*\n",
    "3. Click inside a block to get the details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
