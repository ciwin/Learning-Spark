{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Optimizing and Tuning Spark Applications\n",
    "Christoph Windheuser    \n",
    "May, 2022   \n",
    "Python examples of chapter 7 (page 173 ff) in the book *Learning Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python spark libraries\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a SparkSession\n",
    "\n",
    "spark = (SparkSession \\\n",
    "         .builder \\\n",
    "         .enableHiveSupport() \\\n",
    "         .appName(\"Chapter_7\") \\\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark\r\n"
     ]
    }
   ],
   "source": [
    "# Show the content of the environment variable $SPARK_HOME:\n",
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\r\n",
      "-rw-r--r-- 1 christoph christoph 1105 Jan 20 21:10 fairscheduler.xml.template\r\n",
      "-rw-r--r-- 1 christoph christoph 2471 Mai  5 15:49 log4j.properties\r\n",
      "-rw-r--r-- 1 christoph christoph 2471 Jan 20 21:10 log4j.properties.template\r\n",
      "-rw-r--r-- 1 christoph christoph 9141 Jan 20 21:10 metrics.properties.template\r\n",
      "-rw-r--r-- 1 christoph christoph 1353 Mai  5 15:57 spark-defaults.conf\r\n",
      "-rw-r--r-- 1 christoph christoph 1292 Jan 20 21:10 spark-defaults.conf.template\r\n",
      "-rwxr-xr-x 1 christoph christoph 4428 Jan 20 21:10 spark-env.sh.template\r\n",
      "-rw-r--r-- 1 christoph christoph  865 Jan 20 21:10 workers.template\r\n"
     ]
    }
   ],
   "source": [
    "# Show all config files\n",
    "!ls -l $SPARK_HOME/conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/home/christoph/Dev/LearningSpark/spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "#Get single Spark configuiration values:\n",
    "print(spark.conf.get(\"spark.sql.warehouse.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.jars.packages:\n",
      "org.apache.spark:spark-avro_2.12:3.2.1\n",
      "\n",
      "spark.jars:\n",
      "file:///home/christoph/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.2.1.jar,file:///home/christoph/.ivy2/jars/org.tukaani_xz-1.8.jar,file:///home/christoph/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\n",
      "spark.app.startTime:\n",
      "1652550501773\n",
      "\n",
      "spark.executor.id:\n",
      "driver\n",
      "\n",
      "spark.app.id:\n",
      "local-1652550502663\n",
      "\n",
      "spark.app.name:\n",
      "PySparkShell\n",
      "\n",
      "spark.files:\n",
      "file:///home/christoph/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.2.1.jar,file:///home/christoph/.ivy2/jars/org.tukaani_xz-1.8.jar,file:///home/christoph/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\n",
      "spark.driver.host:\n",
      "192.168.0.125\n",
      "\n",
      "spark.driver.port:\n",
      "35787\n",
      "\n",
      "spark.sql.catalogImplementation:\n",
      "hive\n",
      "\n",
      "spark.rdd.compress:\n",
      "True\n",
      "\n",
      "spark.app.initial.jar.urls:\n",
      "spark://192.168.0.125:35787/jars/org.apache.spark_spark-avro_2.12-3.2.1.jar,spark://192.168.0.125:35787/jars/org.spark-project.spark_unused-1.0.0.jar,spark://192.168.0.125:35787/jars/org.tukaani_xz-1.8.jar\n",
      "\n",
      "spark.submit.pyFiles:\n",
      "/home/christoph/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.2.1.jar,/home/christoph/.ivy2/jars/org.tukaani_xz-1.8.jar,/home/christoph/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\n",
      "spark.app.initial.file.urls:\n",
      "file:///home/christoph/.ivy2/jars/org.tukaani_xz-1.8.jar,file:///home/christoph/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:///home/christoph/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.2.1.jar\n",
      "\n",
      "spark.serializer.objectStreamReset:\n",
      "100\n",
      "\n",
      "spark.master:\n",
      "local[*]\n",
      "\n",
      "spark.submit.deployMode:\n",
      "client\n",
      "\n",
      "spark.repl.local.jars:\n",
      "file:///home/christoph/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.2.1.jar,file:///home/christoph/.ivy2/jars/org.tukaani_xz-1.8.jar,file:///home/christoph/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar\n",
      "\n",
      "spark.sql.warehouse.dir:\n",
      "file:/home/christoph/Dev/LearningSpark/spark-warehouse\n",
      "\n",
      "spark.ui.showConsoleProgress:\n",
      "true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the whole confiuguration context of a Spark Context:\n",
    "scConf = sc.getConf().getAll()\n",
    "\n",
    "for l in scConf:\n",
    "    print (l[0] + \":\")\n",
    "    print (l[1])\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change single Spark config variables\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|key                                                          |value                                                           |\n",
      "+-------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|spark.sql.adaptive.advisoryPartitionSizeInBytes              |<value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>|\n",
      "|spark.sql.adaptive.autoBroadcastJoinThreshold                |<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.enabled                |true                                                            |\n",
      "|spark.sql.adaptive.coalescePartitions.initialPartitionNum    |<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.minPartitionSize       |1MB                                                             |\n",
      "|spark.sql.adaptive.coalescePartitions.parallelismFirst       |true                                                            |\n",
      "|spark.sql.adaptive.customCostEvaluatorClass                  |<undefined>                                                     |\n",
      "|spark.sql.adaptive.enabled                                   |true                                                            |\n",
      "|spark.sql.adaptive.localShuffleReader.enabled                |true                                                            |\n",
      "|spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold      |0b                                                              |\n",
      "|spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled|true                                                            |\n",
      "|spark.sql.adaptive.optimizer.excludedRules                   |<undefined>                                                     |\n",
      "|spark.sql.adaptive.skewJoin.enabled                          |true                                                            |\n",
      "|spark.sql.adaptive.skewJoin.skewedPartitionFactor            |5                                                               |\n",
      "|spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes  |256MB                                                           |\n",
      "|spark.sql.ansi.enabled                                       |false                                                           |\n",
      "|spark.sql.autoBroadcastJoinThreshold                         |10MB                                                            |\n",
      "|spark.sql.avro.compression.codec                             |snappy                                                          |\n",
      "|spark.sql.avro.deflate.level                                 |-1                                                              |\n",
      "|spark.sql.avro.filterPushdown.enabled                        |true                                                            |\n",
      "+-------------------------------------------------------------+----------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the Spark SQL-specifdic Spark configs:\n",
    "spark.sql(\"SET -v\").select(\"key\", \"value\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark's Web Interface\n",
    "To see Spark's Web Interface, go the web address: http://127.0.0.1:4040    \n",
    "The tab *Environment* shows all environment variables. In the web interface, the variables are *read-only*, they cannot be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set configuration variables in a Spark program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First check, if a configuration variable is modifiable:\n",
    "\n",
    "# Example:\n",
    "spark.conf.isModifiable(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the actual value of the variable:\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the variable to a new variable and check:\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set it back to the old value:\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
