{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ea1e65",
   "metadata": {},
   "source": [
    "# Chapter 8: Structured Streaming\n",
    "Christoph Windheuser    \n",
    "May, 2022   \n",
    "Python examples of chapter 8 (page 207 ff) in the book *Learning Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9248326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python spark libraries\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.streaming import StreamingContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d814f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "# This requires access to the internet. If executed offline, an error is thrown\n",
    "\n",
    "spark = (SparkSession \\\n",
    "         .builder \\\n",
    "         .appName(\"Chapter_8\") \\\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42318456",
   "metadata": {},
   "source": [
    "# Create a stream\n",
    "Run the command `nc -lk 9999`in a terminal window.    \n",
    "All text you type into the terminal will be send as a data stream to port 9999 whenever you hit `Return`.    \n",
    "`nc` stands for *Netcat* and is a simple computer network utility available under Linux, macOS and Windows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab5a98",
   "metadata": {},
   "source": [
    "# Example of Reading a stream of data\n",
    "Creating a DataFrame from a text data stream to be received over a socket connection on localhost. Doing a continuous word count on the streaming data and print the results to the console.\n",
    "\n",
    "Whenever text is typed into the `nc` command in the terminal, the text is processed and the word count is printed out in the console of the Jupyter notebook until the spark command `streamingQuery.stop()` is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22114db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = (spark\n",
    "         .readStream.format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", \"9999\")\n",
    "         .load()\n",
    ")\n",
    "\n",
    "words  = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
    "counts = words.groupBy(\"word\").count()\n",
    "\n",
    "# The directory is created if it does not exist\n",
    "checkpointDir = \"/tmp/checkpoints\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b141eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery = (counts\n",
    "                 .writeStream\n",
    "                 .format(\"console\")\n",
    "                 .outputMode(\"complete\")  \n",
    "                 .trigger(processingTime=\"2 second\")\n",
    "                 .option(\"checkpointLocation\", checkpointDir)\n",
    "                 .start()\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de878cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Streaming Query:\n",
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d48cea",
   "metadata": {},
   "source": [
    "## Monitoring an Active Stream\n",
    "Page 223 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010093f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '92b42249-937d-4923-b5bb-11107d147357',\n",
       " 'runId': 'a985dcf1-a11d-4091-961a-2fc406a797cd',\n",
       " 'name': None,\n",
       " 'timestamp': '2022-05-24T11:39:12.000Z',\n",
       " 'batchId': 7,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 0, 'triggerExecution': 0},\n",
       " 'stateOperators': [{'operatorName': 'stateStoreSave',\n",
       "   'numRowsTotal': 3,\n",
       "   'numRowsUpdated': 0,\n",
       "   'allUpdatesTimeMs': 62,\n",
       "   'numRowsRemoved': 0,\n",
       "   'allRemovalsTimeMs': 0,\n",
       "   'commitTimeMs': 13261,\n",
       "   'memoryUsedBytes': 87192,\n",
       "   'numRowsDroppedByWatermark': 0,\n",
       "   'numShufflePartitions': 200,\n",
       "   'numStateStoreInstances': 200,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 2400,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 21424}}],\n",
       " 'sources': [{'description': 'TextSocketV2[host: localhost, port: 9999]',\n",
       "   'startOffset': 6,\n",
       "   'endOffset': 6,\n",
       "   'latestOffset': 6,\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@239fe95e',\n",
       "  'numOutputRows': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show last progress of the stream. Only shows results when the stream is active\n",
    "streamingQuery.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5686170d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the actual status of the stream:\n",
    "streamingQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136df9a",
   "metadata": {},
   "source": [
    "## Other Examples\n",
    "### Example from Sparks Documentation\n",
    "See: https://spark.apache.org/docs/latest/streaming-programming-guide.html     \n",
    "and:\n",
    "https://github.com/apache/spark/blob/v3.2.1/examples/src/main/python/streaming/network_wordcount.py\n",
    "\n",
    "The Python code is in the file `network_wordcount.py`.\n",
    "\n",
    "1. Run the program `nc -lk 9999` in a terminal.    \n",
    "   This program sends all text entered in the terminal out via port 9999\n",
    "2. Run the command `spark-submit network_wordcount.py localhost 9999` in another terminal.\n",
    "3. Each time words are typed in the first terminal, the words are counted in the second terminal\n",
    "4. Terminate both commands with `Cntr-C`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7097278",
   "metadata": {},
   "source": [
    "### Example from the Blog *Apache Spark Structured Streaming with Pyspark* from Sercan Karagoz\n",
    "**Windowed Streaming:**\n",
    "\n",
    "See: [Link to the blog on Medium](https://medium.com/analytics-vidhya/apache-spark-structured-streaming-with-pyspark-b4a054a7947d)\n",
    "\n",
    "The scenario is that in a retail store sold articles are entered into the terminal whenever they are sold. The article names are transported as a stream to Spark and a Spark query groups the input data by article and counts them and writes the result to the console. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63437ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = (spark\n",
    "           .readStream\n",
    "           .format(\"socket\")\n",
    "           .option(\"host\", \"localhost\")\n",
    "           .option(\"port\", \"9999\")\n",
    "           .option(\"includeTimestamp\", True)\n",
    "           .load()\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "585f0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (rawdata\n",
    "         .select((rawdata.value).alias(\"product\"),(rawdata.timestamp).alias(\"time\"))\n",
    "         .groupBy(window(\"time\", \"1 minutes\"), \"product\")\n",
    "         .count().sort(desc(\"window\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b23f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (query.writeStream\n",
    "          .outputMode(\"complete\")\n",
    "          .format(\"console\")\n",
    "          .start(truncate=False)\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "993c2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Streaming Query:\n",
    "result.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9c7e802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the actual status of the stream:\n",
    "result.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10c8a8",
   "metadata": {},
   "source": [
    "# Streaming Data Sources and Sinks\n",
    "Page 226 ff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6dc83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dir /tmp/sparkInputDir\n",
    "\n",
    "inputDirectoryOfFiles = \"/tmp/sparkInputDir\"\n",
    "\n",
    "fileSchema = (StructType()\n",
    "             .add(StructField(\"key\", IntegerType()))\n",
    "             .add(StructField(\"value\", StringType()))\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9274d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF =(spark\n",
    "          .readStream\n",
    "          .format(\"csv\")\n",
    "          .schema(fileSchema)\n",
    "          .option(\"maxFilesPerTrigger\", 1)\n",
    "          .load(inputDirectoryOfFiles)\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d6a54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a32108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: integer (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ecf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery = (inputDF\n",
    "                 .writeStream\n",
    "                 .format(\"console\")\n",
    "                 .outputMode(\"append\")\n",
    "                 .start()\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b21871",
   "metadata": {},
   "source": [
    "Now copy the files `streamingData_xx.csv` one by one from the directory `data/streamingData` to the direcory `/tmp/sparkInputDir`. Everytime a file is copied, the content is read by the `readStream` and written to the console by the `writeStream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97d50c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop streaming\n",
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259e6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
