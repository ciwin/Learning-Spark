{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Building Reliable Data Lakes with Apache Spark\n",
    "Christoph Windheuser    \n",
    "May, 2022   \n",
    "Python examples of chapter 9 (page 265 ff) in the book *Learning Spark*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use delta-lake functionality in PySpark, add the following line to the `spark-defaults.conf` file in `$SPARK_HOME/conf` directory:\n",
    "\n",
    "```spark.jars.packages io.delta:delta-core_2.12:1.1.0```\n",
    "\n",
    "If you want to add more jar packages, separate them with comma **without blanks**!\n",
    "\n",
    "If no file `spark-defaults.conf` exists in `$SPARK_HOME/conf` but a file `spark-defaults.conf.template`, then copy this template file to `spark-defaults.conf` and do the changes in the new file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python spark libraries\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from delta import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Delta Lake from Jupyter Notebooks:\n",
    "See Stackoverflow: https://stackoverflow.com/questions/57740693/how-to-refer-deltalake-tables-in-jupyter-notebook-using-pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a SparkSession\n",
    "# This requires access to the internet. If executed offline, an error is thrown\n",
    "\n",
    "builder = (SparkSession \\\n",
    "         .builder \\\n",
    "         .appName(\"Chapter_9\") \\\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into a Delta Lake Table\n",
    "Page 275 ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcePath = \"../DB_Spark/LearningSparkV2/databricks-datasets/learning-spark-v2/loans/loan-risks.snappy.parquet\"\n",
    "\n",
    "deltaPath  = \"/tmp/spark/loans_delta\"\n",
    "\n",
    "(spark.read.format(\"parquet\").load(sourcePath)\n",
    "    .write.format(\"delta\").save(deltaPath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   14705|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the rows:\n",
    "spark.sql(\"SELECT count(*) FROM loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
      "+-------+-----------+---------+----------+\n",
      "|      0|       1000|   182.22|        CA|\n",
      "|      1|       1000|   361.19|        WA|\n",
      "|      2|       1000|   176.26|        TX|\n",
      "|      3|       1000|   1000.0|        OK|\n",
      "|      4|       1000|   249.98|        PA|\n",
      "+-------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows of the table:\n",
    "spark.sql(\"SELECT * FROM loans_delta LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforcing Schema on Write to Prevent Data Coruption\n",
    "Page 278\n",
    "\n",
    "When we try to append new rows with another schema (the row 'closed' was added) to the existing delta table, Spark throws an error *A schema mismatch detected*. The error message shows the two different schemas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 21ee7030-632b-4306-92b7-9ca406724adb).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-cd2a62c15e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m               .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloanUpdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltaPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 21ee7030-632b-4306-92b7-9ca406724adb).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "cols  = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n",
    "items = [(1111111, 1000, 1000.0, 'TX', True),\n",
    "        (2222222, 2000, 0.0,    'CA', False)]\n",
    "\n",
    "loanUpdates = (spark.createDataFrame(items, cols)\n",
    "              .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))\n",
    "\n",
    "loanUpdates.write.format(\"delta\").mode(\"append\").save(deltaPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Migration\n",
    "Page 279\n",
    "If we want to adapt the original schema to the new schema, we can do this with setting `mergeSchema`to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loanUpdates.write.format(\"delta\").mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(deltaPath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|      0|       1000|   182.22|        CA|  null|\n",
      "|      1|       1000|   361.19|        WA|  null|\n",
      "|      2|       1000|   176.26|        TX|  null|\n",
      "|      3|       1000|   1000.0|        OK|  null|\n",
      "|      4|       1000|   249.98|        PA|  null|\n",
      "+-------+-----------+---------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows of the new table:\n",
    "loanUpdatesNew = spark.read.format(\"delta\").load(deltaPath)\n",
    "loanUpdatesNew.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Data\n",
    "Page 280\n",
    "\n",
    "Change all Adress States 'WA' to 'OR':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.update(\"addr_state = 'WA'\", {\"addr_state\": \"'OR'\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|      0|       1000|   182.22|        CA|  null|\n",
      "|      1|       1000|   361.19|        OR|  null|\n",
      "|      2|       1000|   176.26|        TX|  null|\n",
      "|      3|       1000|   1000.0|        OK|  null|\n",
      "|      4|       1000|   249.98|        PA|  null|\n",
      "+-------+-----------+---------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows of the new table:\n",
    "loanUpdatesNew = spark.read.format(\"delta\").load(deltaPath)\n",
    "loanUpdatesNew.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Data\n",
    "Page 280\n",
    "\n",
    "Delete all rows where the loan is fully paid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "deltaTable.delete(\"funded_amnt <= paid_amnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|      0|       1000|   182.22|        CA|  null|\n",
      "|      1|       1000|   361.19|        OR|  null|\n",
      "|      2|       1000|   176.26|        TX|  null|\n",
      "|      4|       1000|   249.98|        PA|  null|\n",
      "|      5|       1000|    408.6|        CA|  null|\n",
      "+-------+-----------+---------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows of the new table:\n",
    "loanUpdatesNew = spark.read.format(\"delta\").load(deltaPath)\n",
    "loanUpdatesNew.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally delete the delta table to be able to create it again next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the delta tables on the file system:\n",
    "!rm /tmp/spark/loans_delta/*.parquet\n",
    "!rm -rf /tmp/spark/loans_delta/_delta_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
