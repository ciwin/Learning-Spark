{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Higher-Order Functions\n",
    "Christoph Windheuser    \n",
    "May, 2022   \n",
    "Python examples of chapter 5 Higher-Order Functions (page 141 ff) in the book *Learning Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python spark libraries\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "# from pyspark.sql.functions import col, expr, when, concat, lit, avg, desc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use when running a Jupyter Notebook, don't use it when starting with pyspark!\n",
    "# Connect Jupyter Notebook with the Spark application and create Spark Context\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName=\"chapter_4_HOF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a SparkSession\n",
    "\n",
    "spark = (SparkSession \\\n",
    "         .builder \\\n",
    "         .enableHiveSupport() \\\n",
    "         .config(\"spark.sql.catalogImplementation\",\"hive\") \\\n",
    "         .appName(\"Chapter_4_HOF\") \\\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = spark.createDataFrame(t_list, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c.createOrReplaceTempView(\"tC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Higher-Order Function *transform()*\n",
    "The `transform()` function produces an array by applying a function to each element of the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit\n",
    "FROM tC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *filter()*\n",
    "The function `filter()` produces an array consisting of only the elements of the input array for which the Boolean funciton is `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter all temperatures > 38C for an array of temperatures\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "filter(celsius, t -> t > 38) as high\n",
    "FROM tC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *exists()*\n",
    "The function `exists()` returns `true` if the boolean function holds for any element in the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "       exists (celsius, t -> t = 38) as threshold\n",
    "FROM tC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *aggregate()*\n",
    "In the book, the function `reduce()` is used, but throws an error. The function `aggregate()` provides the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|             celsius|avgFahrenheit|\n",
      "+--------------------+-------------+\n",
      "|[35, 36, 32, 30, ...|           96|\n",
      "|[31, 32, 34, 55, 56]|          105|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average temperature of a row and convert it to Fahrenheit\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "    aggregate(\n",
    "        celsius,\n",
    "        0,\n",
    "        (t, acc) -> t + acc,\n",
    "        acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    "        ) as avgFahrenheit\n",
    "FROM tC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common DataFrames and Spark SQL Operations\n",
    "Page 144 ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "tripdelaysFilePath = \"\"\"../DB_Spark/LearningSparkV2/databricks-datasets/\\\n",
    "learning-spark-v2/flights/departuredelays.csv\"\"\"\n",
    "\n",
    "airportsnaFilePath = \"\"\"../DB_Spark/LearningSparkV2/databricks-datasets/\\\n",
    "learning-spark-v2/flights/airport-codes-na.txt\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain airport data set\n",
    "airportsna = (spark.read\n",
    "             .format(\"csv\")\n",
    "             .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    "             .load(airportsnaFilePath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view from the DataFrame\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain departure delays data set\n",
    "departureDelays = (spark.read\n",
    "                  .format(\"csv\")\n",
    "                  .options(header=\"true\")\n",
    "                  .load(tripdelaysFilePath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast delay and distance to INT\n",
    "departureDelays =(departureDelays\n",
    "                 .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "                 .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view from the DataFrame\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary small table\n",
    "foo = (departureDelays\n",
    "      .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and\n",
    "      date like '01010%' and delay > 0\"\"\")))\n",
    "\n",
    "foo.createOrReplaceTempView(\"foo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the table airports_na\n",
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the table departureDelays\n",
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the table foo\n",
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unions\n",
    "Page 147 ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the two tables \"deparetureDelays\" and \"foo\" and creating a new table \"bar\"\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for SEA and SFO in the specific time range - the same filter criteria as for foo.\n",
    "# This will result in 2x the entries of \"foo\", as these entries are part of the tables\n",
    "# \"departureDelays\" and \"foo\"\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "                    AND date LIKE '01010%' AND delay > 0\"\"\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "Page 148\n",
    "\n",
    "By default, a Spark SQL join is an inner join, but other options are possible (see documentation).\n",
    "\n",
    "The following code performs an inner joins between the two DataFrames `airportsna` and `foo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(\n",
    "    airportsna,\n",
    "    airportsna.IATA == foo.origin\n",
    "    ).select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS departureDelaysWindow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE departureDelaysWindow AS \\\n",
    "    SELECT origin, destination, SUM(delay) AS TotalDelays \\\n",
    "    FROM   departureDelays \\\n",
    "    WHERE  origin IN('SEA','SFO','JFK') \\\n",
    "    AND    destination IN('SEA','SFO','JFK','DEN','ORD','LAX','ATL') \\\n",
    "    GROUP  BY origin, destination;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|TotalDelays|\n",
      "+------+-----------+-----------+\n",
      "|   JFK|        ORD|       5608|\n",
      "|   JFK|        SFO|      35619|\n",
      "|   JFK|        DEN|       4315|\n",
      "|   JFK|        ATL|      12141|\n",
      "|   JFK|        SEA|       7856|\n",
      "|   JFK|        LAX|      35755|\n",
      "|   SEA|        LAX|       9359|\n",
      "|   SFO|        ORD|      27412|\n",
      "|   SFO|        DEN|      18688|\n",
      "|   SFO|        SEA|      17080|\n",
      "|   SEA|        SFO|      22293|\n",
      "|   SFO|        ATL|       5091|\n",
      "|   SEA|        DEN|      13645|\n",
      "|   SEA|        ATL|       4535|\n",
      "|   SEA|        ORD|      10041|\n",
      "|   SFO|        JFK|      24100|\n",
      "|   SFO|        LAX|      40798|\n",
      "|   SEA|        JFK|       4667|\n",
      "+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelaysWindow\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the window function `dense_ran()` to caluclate the tree top delayed destinations per origin airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+----+\n",
      "|origin|destination|TotalDelays|rank|\n",
      "+------+-----------+-----------+----+\n",
      "|   JFK|        LAX|      35755|   1|\n",
      "|   JFK|        SFO|      35619|   2|\n",
      "|   JFK|        ATL|      12141|   3|\n",
      "|   SEA|        SFO|      22293|   1|\n",
      "|   SEA|        DEN|      13645|   2|\n",
      "|   SEA|        ORD|      10041|   3|\n",
      "|   SFO|        LAX|      40798|   1|\n",
      "|   SFO|        ORD|      27412|   2|\n",
      "|   SFO|        JFK|      24100|   3|\n",
      "+------+-----------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT origin, destination, TotalDelays, rank\n",
    "FROM (\n",
    "    SELECT origin, destination, TotalDelays, dense_rank()\n",
    "    OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank\n",
    "    FROM departureDelaysWindow\n",
    "    ) t\n",
    "    WHERE rank <= 3\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications\n",
    "Page 151 ff\n",
    "\n",
    "### Adding new columns\n",
    "Start with our small DataFrame example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column to the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo2 = (foo.withColumn(\n",
    "        \"status\",\n",
    "        expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting\n",
    "Pivoting means swapping the columns for the rows in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_SEA_df = spark.sql(\"\"\"\n",
    "    SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "    FROM   departureDelays\n",
    "    WHERE  origin = 'SEA'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "|        ORD|    1|    0|\n",
      "|        DFW|    1|   23|\n",
      "|        DFW|    1|   36|\n",
      "|        ORD|    1|  298|\n",
      "|        JFK|    1|    4|\n",
      "|        DFW|    1|    0|\n",
      "|        MIA|    1|    2|\n",
      "|        DFW|    1|    0|\n",
      "|        DFW|    1|    0|\n",
      "|        ORD|    1|   83|\n",
      "+-----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from_SEA_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_SEA_df2 = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "    SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay\n",
    "    FROM   departureDelays WHERE  origin = 'SEA'\n",
    "    )\n",
    "    PIVOT (\n",
    "        CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "        FOR month IN (1 JAN, 2 FEB)\n",
    "    )\n",
    "    ORDER BY destination\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from_SEA_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
